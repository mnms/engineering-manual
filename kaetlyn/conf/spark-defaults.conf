#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
#spark.master                           yarn
#spark.eventLog.enabled                 true
#spark.eventLog.dir                     hdfs://namenode:8021/directory
#spark.scheduler.pool                   nvkvs_pool
#spark.serializer                       org.apache.spark.serializer.KryoSerializer
#spark.executor.extraJavaOptions        -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
#
#

#spark.master                           yarn
#spark.sql.hive.metastore.version       1.2.1
#hive.metastore.warehouse.dir           hdfs://tg-p-anv-nd01:15000/user/nvkvs/spark-warehouse
#spark.sql.warehouse.dir            h   dfs://tg-p-anv-nd01:15000/user/nvkvs/spark-warehouse

spark.scheduler.mode                    FAIR
#spark.scheduler.allocation.file         /home/nvkvs/spark/conf/scheduler-site.xml
spark.scheduler.cluster.fair.pool       default
spark.kryoserializer.buffer.max	        2047m
spark.driver.maxResultSize              4g
#spark.local.dir                         /data21/spark,/data22/spark
hive.server2.thrift.port                13100
spark.memory.fraction 0.3

spark.serializer                        org.apache.spark.serializer.KryoSerializer

spark.memory.offHeap.size               5g
spark.memory.offHeap.enabled            true

#spark.io.compression.codec              lz4
#todo check this setting is needed
#spark.rdd.compress                      true

#add for GC prevention
spark.yarn.executor.memoryOverhead      4096
spark.yarn.driver.memoryOverhead        6000

spark.cores.max                         60

spark.network.timeout                   60s

spark.ui.enabled                        true
spark.ui.retainedJobs                   2
spark.ui.retainedStages                 10
spark.ui.retainedTasks                  1000000

spark.worker.ui.retainedExecutors       2
spark.worker.ui.retainedDrivers         2
spark.sql.ui.retainedExecutions         2

#spark.eventLog.enabled                  true
#spark.eventLog.dir                      file:///home/nvkvs/nvkvs/spark/event-log
#spark.eventLog.dir                      file:///data22/thriftserver-event-logs

#spark.history.fs.logDirectory           hdfs://d01:15000/spark/event-logs
#spark.history.fs.logDirectory           file:///home/nvkvs/nvkvs/spark/event-log
#spark.history.fs.logDirectory           file:///data22/thriftserver-event-logs

#spark.sql.shuffle.partitions            250
#spark.sql.hive.thriftServer.singleSession true
spark.executor.heartbeatInterval        30s
#spark.yarn.am.extraJavaOptions          '-XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=35 -XX:ConcGCThreads=20'
#spark.executor.extraJavaOptions         -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=25 -XX:ParallelGCThreads=20 -XX:ConcGCThreads=5 -XX:MaxGCPauseMillis=100 -XX:+UseStringDeduplication
#spark.executor.extraJavaOptions         -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=25 -XX:ParallelGCThreads=20 -XX:ConcGCThreads=5
#spark.driver.extraJavaOptions           -XX:+UseG1GC -XX:InitiatingHeapOccupancyPercent=25 -XX:ParallelGCThreads=20 -XX:ConcGCThreads=5 -XX:OnOutOfMemoryError="kill -9 %p"
spark.executor.extraJavaOptions         -XX:ParallelGCThreads=20
#spark.driver.extraJavaOptions           -XX:ParallelGCThreads=20 -XX:OnOutOfMemoryError="kill -9 %p" -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/data22/spark/heap_dump
spark.driver.extraJavaOptions           -XX:ParallelGCThreads=20 -XX:OnOutOfMemoryError="kill -9 %p" -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/nvdrive0/spark/heap_dump

#spark.oom.maxusagepercent               85
